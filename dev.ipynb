{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "from torch_geometric.data import Data\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, game_manager_args, simulation_manager_args, model_args, plot=False):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        logger.info(\"Initializing CustomEnv\")\n",
    "        # ... (previous initialization code remains the same)\n",
    "        \n",
    "        self.max_episode_steps = 1000  # Maximum number of steps per episode\n",
    "        self.episode_step = 0\n",
    "        self.total_reward = 0\n",
    "        logger.info(\"CustomEnv initialized successfully\")\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        logger.info(\"Resetting environment\")\n",
    "        self.episode_step = 0\n",
    "        self.total_reward = 0\n",
    "        self.outposts_visited.clear()\n",
    "        self.early_stop = False\n",
    "        # ... (rest of the reset code remains the same)\n",
    "        logger.info(f\"Environment reset complete. Initial agent position: {(self.agent_controler.agent.grid_x, self.agent_controler.agent.grid_y)}\")\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        logger.debug(f\"Taking step {self.episode_step} with action {action}\")\n",
    "        self.episode_step += 1\n",
    "        \n",
    "        prev_position = (self.agent_controler.agent.grid_x, self.agent_controler.agent.grid_y)\n",
    "        self.agent_controler.agent_action(action)\n",
    "        new_position = (self.agent_controler.agent.grid_x, self.agent_controler.agent.grid_y)\n",
    "        \n",
    "        self.current_gm.rerender()\n",
    "        reward = self._calculate_reward()\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        terminated = self.early_stop or self.episode_step >= self.max_episode_steps\n",
    "        truncated = self.episode_step >= self.max_episode_steps\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        info = {\n",
    "            \"episode_step\": self.episode_step,\n",
    "            \"prev_position\": prev_position,\n",
    "            \"new_position\": new_position,\n",
    "            \"energy_spent\": self.agent_controler.energy_spent,\n",
    "            \"outposts_visited\": len(self.outposts_visited),\n",
    "            \"total_reward\": self.total_reward\n",
    "        }\n",
    "        \n",
    "        logger.debug(f\"Step complete. Reward: {reward}, Total Reward: {self.total_reward}, Terminated: {terminated}, Truncated: {truncated}, Info: {info}\")\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            logger.info(f\"Episode ended. Total steps: {self.episode_step}, Total reward: {self.total_reward}, Outposts visited: {len(self.outposts_visited)}/{len(self.outpost_coords)}\")\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        logger.debug(\"Calculating reward\")\n",
    "        agent_pos = (self.agent_controler.agent.grid_x, self.agent_controler.agent.grid_y)\n",
    "        reward = 0\n",
    "\n",
    "        if agent_pos in self.outpost_coords and agent_pos not in self.outposts_visited:\n",
    "            reward += self.new_outpost_reward\n",
    "            self.outposts_visited.add(agent_pos)\n",
    "            logger.debug(f\"Agent reached new outpost. Reward: {self.new_outpost_reward}\")\n",
    "\n",
    "            if len(self.outposts_visited) == len(self.outpost_coords):\n",
    "                reward += self.completion_reward\n",
    "                logger.debug(f\"All outposts visited. Additional reward: {self.completion_reward}\")\n",
    "                self.early_stop = True\n",
    "\n",
    "        else:\n",
    "            unvisited_outposts = [outpost for outpost in self.outpost_coords if outpost not in self.outposts_visited]\n",
    "            if unvisited_outposts:\n",
    "                prev_min_distance = self.previous_min_distance if hasattr(self, 'previous_min_distance') else float('inf')\n",
    "                current_min_distance = min(manhattan_distance(agent_pos, outpost) for outpost in unvisited_outposts)\n",
    "                \n",
    "                if current_min_distance < prev_min_distance:\n",
    "                    reward += self.closer_to_outpost_reward\n",
    "                    logger.debug(f\"Agent moved closer to an outpost. Reward: {self.closer_to_outpost_reward}\")\n",
    "                elif current_min_distance > prev_min_distance:\n",
    "                    reward += self.farther_from_outpost_penalty\n",
    "                    logger.debug(f\"Agent moved away from outposts. Penalty: {self.farther_from_outpost_penalty}\")\n",
    "                \n",
    "                self.previous_min_distance = current_min_distance\n",
    "            else:\n",
    "                reward += self.penalty\n",
    "                logger.debug(f\"No unvisited outposts. Penalty: {self.penalty}\")\n",
    "\n",
    "        logger.debug(f\"Calculated reward: {reward}, Outposts visited: {len(self.outposts_visited)}/{len(self.outpost_coords)}\")\n",
    "        return reward\n",
    "\n",
    "    # ... (rest of the methods remain the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from custom_env import CustomEnv\n",
    "from agent_model import AgentModel\n",
    "import cProfile\n",
    "import pstats\n",
    "import logging\n",
    "import traceback\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Capture warnings\n",
    "warnings.filterwarnings(\"always\")\n",
    "\n",
    "os.environ['PYGAME_DETECT_AVX2'] = '1'\n",
    "\n",
    "# ... (rest of the setup code remains the same)\n",
    "\n",
    "def calculate_mean_reward(ep_info_buffer):\n",
    "    if len(ep_info_buffer) > 0:\n",
    "        return np.mean([ep_info[\"r\"] for ep_info in ep_info_buffer])\n",
    "    return 0.0\n",
    "\n",
    "def calculate_mean_episode_length(ep_info_buffer):\n",
    "    if len(ep_info_buffer) > 0:\n",
    "        return np.mean([ep_info[\"l\"] for ep_info in ep_info_buffer])\n",
    "    return 0.0\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logger.info(\"Starting main function\")\n",
    "        \n",
    "        # ... (rest of the setup code remains the same)\n",
    "\n",
    "        # Run the training\n",
    "        logger.info(\"Starting model training\")\n",
    "        total_timesteps = 1000000\n",
    "        timeout = 3600  # 1 hour timeout\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            for i in range(0, total_timesteps, 2048):  # 2048 is the n_steps parameter\n",
    "                logger.info(f\"Training iteration {i//2048 + 1}, timesteps {i}-{min(i+2048, total_timesteps)}\")\n",
    "                \n",
    "                # Check for timeout\n",
    "                if time.time() - start_time > timeout:\n",
    "                    logger.warning(\"Training timed out after 1 hour\")\n",
    "                    break\n",
    "\n",
    "                with warnings.catch_warnings(record=True) as w:\n",
    "                    rl_model.learn(total_timesteps=2048, callback=eval_callback, reset_num_timesteps=False)\n",
    "                    if len(w) > 0:\n",
    "                        logger.warning(f\"Warnings during training: {[str(warn.message) for warn in w]}\")\n",
    "\n",
    "                logger.info(f\"Completed training iteration {i//2048 + 1}\")\n",
    "\n",
    "                # Log some training statistics\n",
    "                mean_reward = calculate_mean_reward(rl_model.ep_info_buffer)\n",
    "                mean_episode_length = calculate_mean_episode_length(rl_model.ep_info_buffer)\n",
    "                logger.info(f\"Recent mean reward: {mean_reward:.2f}\")\n",
    "                logger.info(f\"Recent mean episode length: {mean_episode_length:.2f}\")\n",
    "\n",
    "                # Log the size of the episode info buffer\n",
    "                logger.info(f\"Episode info buffer size: {len(rl_model.ep_info_buffer)}\")\n",
    "\n",
    "                # Log a sample of the episode info buffer\n",
    "                if len(rl_model.ep_info_buffer) > 0:\n",
    "                    logger.info(f\"Sample episode info: {rl_model.ep_info_buffer[-1]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "        \n",
    "        logger.info(\"Model training completed or stopped\")\n",
    "\n",
    "        # ... (rest of the evaluation and cleanup code remains the same)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in the main function: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "from torch_geometric.data import Data\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, game_manager_args, simulation_manager_args, model_args, plot=False):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        logger.info(\"Initializing CustomEnv\")\n",
    "        # ... (rest of the initialization code remains the same)\n",
    "        self.step_count = 0\n",
    "        logger.info(\"CustomEnv initialized successfully\")\n",
    "\n",
    "    # ... (other methods remain the same)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        logger.info(\"Resetting environment\")\n",
    "        self.step_count = 0\n",
    "        # ... (rest of the reset code remains the same)\n",
    "        logger.info(f\"Environment reset complete. Initial observation: {observation}\")\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        logger.debug(f\"Taking step {self.step_count} with action {action}\")\n",
    "        self.step_count += 1\n",
    "        self.agent_controler.agent_action(action)\n",
    "        self.current_gm.rerender()\n",
    "        reward = self._calculate_reward()\n",
    "        terminated = self.early_stop\n",
    "        truncated = False\n",
    "        observation = self._get_observation()\n",
    "        info = {\n",
    "            \"step_count\": self.step_count,\n",
    "            \"agent_position\": (self.agent_controler.agent.grid_x, self.agent_controler.agent.grid_y),\n",
    "            \"energy_spent\": self.agent_controler.energy_spent,\n",
    "            \"outposts_visited\": len(self.outposts_visited)\n",
    "        }\n",
    "        logger.debug(f\"Step complete. Reward: {reward}, Terminated: {terminated}, Truncated: {truncated}, Info: {info}\")\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        logger.debug(\"Calculating reward\")\n",
    "        # ... (rest of the method code remains the same)\n",
    "        logger.debug(f\"Calculated reward: {self.current_reward}, Outposts visited: {len(self.outposts_visited)}/{len(self.outpost_coords)}\")\n",
    "        return self.current_reward\n",
    "\n",
    "    # ... (rest of the class methods remain the same)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
